{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PBBrGwtqJ_v"
      },
      "source": [
        "# Assignment 3: Image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XipS0rz6SPdf"
      },
      "source": [
        "## Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ymqi_BhqTfL"
      },
      "source": [
        "In this assignment, you will be working on the COVID-19 lung X-Ray dataset. It consists of X-ray images that are labeled as either \"Normal\" or \"Covid\", with the latter indicating an X-ray of a COVID-19 patient.\n",
        "\n",
        "To predict if a person has COVID or not, you will train the following five different classification models, finetune their hyperparameters, and finally select the best model and test it on the test data.\n",
        "\n",
        "1.   Decision Tree\n",
        "2.   Gradient Boosting\n",
        "3.   Random Forest\n",
        "4.   Support Vector Machines\n",
        "5.   Neural Netwroks\n",
        "\n",
        "The dataset can be found here: [Covid 19 X-Ray Dataset](https://drive.google.com/drive/folders/1Nwa6L58NwF23PvImpDrp-W8oTOheu_b3?usp=sharing). The data is already split into training, validation, and test.\n",
        "\n",
        "To help you with this assignment, you are provided some instructions to follow, we will be checking those criteria when evaluating your submissions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU-yk27FSUqW"
      },
      "source": [
        "## Part 1: Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3SFuMXfqGpK"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-Q5D3C1iVw1"
      },
      "outputs": [],
      "source": [
        "# define the path for training, testing and validation datasets\n",
        "train_dir = \"/content/drive/MyDrive/Classroom/Training\"\n",
        "val_dir = \"/content/drive/MyDrive/Classroom/Validation\"\n",
        "test_dir = \"/content/drive/MyDrive/Classroom/Testing\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z847u44djVcR"
      },
      "source": [
        "Since we are dealing with folders and subfolders, it's crucial to use the os library so that we will be able to use the\n",
        "\n",
        "```\n",
        "# os.path.join(folder, 'destination directory name')\n",
        "```\n",
        "\n",
        "where the destination will be either \"COVID\" or \"Normal\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJ444juGipC_"
      },
      "outputs": [],
      "source": [
        "# import libaries\n",
        "import os\n",
        "\n",
        "# Function to count samples in each folder\n",
        "def count_samples(folder):\n",
        "    covid_path = os.path.join(folder, 'COVID')\n",
        "    normal_path = os.path.join(folder, 'Normal')\n",
        "    num_covid = len(os.listdir(covid_path))\n",
        "    num_normal = len(os.listdir(normal_path))\n",
        "    return num_covid, num_normal\n",
        "\n",
        "# count samples in each folder\n",
        "train_covid, train_normal = count_samples(train_dir)\n",
        "test_covid, test_normal = count_samples(test_dir)\n",
        "val_covid, val_normal = count_samples(val_dir)\n",
        "\n",
        "# output the counts\n",
        "print(\"Number of samples in each folder:\")\n",
        "print(\"Number of samples in each folder:\")\n",
        "print(\"Training - COVID:\", train_covid)\n",
        "print(\"Training - Normal:\", train_normal)\n",
        "print(\"Validation - COVID:\", val_covid)\n",
        "print(\"Validation - Normal:\", val_normal)\n",
        "print(\"Testing - COVID:\", test_covid)\n",
        "print(\"Testing - Normal:\", test_normal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycXVdSt4k8I-"
      },
      "source": [
        "Your next step is to visualize some of the data that you have. When we were dealing with a dataframe, it was easier for us to visualize some samples since the dataframe have a head and many other functions to get the job done.\n",
        "\n",
        "In our case, we are dealing with images. The simplest option is to check the folder from your drive but we need to see it inside the model here.\n",
        "\n",
        "To do that, you should use the cv2 library and use the method\n",
        "\n",
        "```\n",
        "cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "```\n",
        "so that we can read an image that is in grayscale.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBOTcinkmVXg"
      },
      "source": [
        "In addition, the image should be in a viewable format, so we need to plot then show the image using the matplotlib library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xPTZMNfkOz5"
      },
      "outputs": [],
      "source": [
        "# import libaries\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# function to visualize images from each folder\n",
        "def visualize_images(folder, label, num_samples=2):\n",
        "    covid_path = os.path.join(folder, 'COVID')\n",
        "    normal_path = os.path.join(folder, 'Normal')\n",
        "\n",
        "    # visualize COVID images\n",
        "    covid_images = os.listdir(covid_path)[:num_samples]\n",
        "    print(f\"\\nVisualizing {num_samples} COVID images from {label} folder:\")\n",
        "    for img_name in covid_images:\n",
        "        img_path = os.path.join(covid_path, img_name)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        plt.imshow(img)\n",
        "        plt.title(\"COVID\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    # visualize Normal images\n",
        "    normal_images = os.listdir(normal_path)[:num_samples]\n",
        "    print(f\"\\nVisualizing {num_samples} Normal images from {label} folder:\")\n",
        "    for img_name in normal_images:\n",
        "        img_path = os.path.join(normal_path, img_name)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        plt.imshow(img)\n",
        "        plt.title(\"Normal\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# visualize images from each folder\n",
        "visualize_images(train_dir, \"Training\")\n",
        "visualize_images(test_dir, \"Testing\")\n",
        "visualize_images(val_dir, \"Validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgi4HBWouRpl"
      },
      "source": [
        "## Part 2: Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZv40q8uqI_R"
      },
      "source": [
        "Since we are dealing with images and folders, and since we know that we do not have a dataframe to choose from, we need to present our folders and images in a suitable format for the different models.\n",
        "\n",
        "To do that, we will be use the method\n",
        "\n",
        "```\n",
        "load_images_from_folder()\n",
        "```\n",
        "that will take as parameters the folder path and the label, and return 2 arrays, one for the features (which are the images) and one for the labels.\n",
        "\n",
        "To build this method, we need at first to initiate 2 empty arrays to hold the new values that we will be working with and we need to make a dictionnary to map the labels COVID and Normal to numeric values (0 and 1).\n",
        "\n",
        "Next, we need to loop over the images inside each folder and do the following:\n",
        "\n",
        "1.   Read the image\n",
        "2.   Convert each image to grayscale\n",
        "3.   Normalize the pixels values between 0 and 1\n",
        "4.   Flatten the image\n",
        "5.   Append the image and its label to the correspondng arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51QhTpzRqC-f"
      },
      "outputs": [],
      "source": [
        "# import libaries\n",
        "import numpy as np\n",
        "\n",
        "def load_images_from_folder(folder_path, label, target_size=(100, 100)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_dict = {'COVID': 0, 'Normal': 1}  # Define a dictionary to map labels to numeric values\n",
        "    label_code = label_dict[label]\n",
        "\n",
        "    label_path = os.path.join(folder_path, label)\n",
        "    for img_name in os.listdir(label_path):\n",
        "        img_path = os.path.join(label_path, img_name)\n",
        "        # load the image\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read the image in grayscale\n",
        "        # resize the image\n",
        "        img_resized = cv2.resize(img, target_size)  # Resize the image to the specified target size\n",
        "        # preprocess the resized image (e.g., normalize pixel values)\n",
        "        img_resized = img_resized.astype('float32') / 255.0  # Normalize pixel values to [0, 1]\n",
        "        # flatten the resized image\n",
        "        img_flat = img_resized.flatten()\n",
        "        # append the flattened image and corresponding label\n",
        "        images.append(img_flat)\n",
        "        labels.append(label_code)\n",
        "    return np.array(images), np.array(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeLmmBv4s63S"
      },
      "source": [
        "Next, call this method for all the folders you have to prepare your data for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juUZmdBKskdq"
      },
      "outputs": [],
      "source": [
        "# load images from the training folder for both COVID and Normal labels\n",
        "train_covid_images, train_covid_labels = load_images_from_folder(train_dir, 'COVID')\n",
        "train_normal_images, train_normal_labels = load_images_from_folder(train_dir, 'Normal')\n",
        "\n",
        "# load images from the testing folder for both COVID and Normal labels\n",
        "test_covid_images, test_covid_labels = load_images_from_folder(test_dir, 'COVID')\n",
        "test_normal_images, test_normal_labels = load_images_from_folder(test_dir, 'Normal')\n",
        "\n",
        "# load images from the validation folder for both COVID and Normal labels\n",
        "val_covid_images, val_covid_labels = load_images_from_folder(val_dir, 'COVID')\n",
        "val_normal_images, val_normal_labels = load_images_from_folder(val_dir, 'Normal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u7mOYTgtiIP"
      },
      "outputs": [],
      "source": [
        "train_covid_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1Is0M4JXY8e"
      },
      "outputs": [],
      "source": [
        "# combine COVID and Normal images and labels for training, validation and test\n",
        "train_images = np.concatenate((train_covid_images, train_normal_images), axis=0)\n",
        "train_labels = np.concatenate((train_covid_labels, train_normal_labels), axis=0)\n",
        "\n",
        "val_images = np.concatenate((val_covid_images, val_normal_images), axis=0)\n",
        "val_labels = np.concatenate((val_covid_labels, val_normal_labels), axis=0)\n",
        "\n",
        "test_images = np.concatenate((test_covid_images, test_normal_images), axis=0)\n",
        "test_labels = np.concatenate((test_covid_labels, test_normal_labels), axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpoBXQ5WAn77"
      },
      "source": [
        "## Part 3: Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYZTGAqsA1xn"
      },
      "source": [
        "\n",
        "### Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX4oDuGxGalj"
      },
      "source": [
        "Train a Decision Tree model using the default hyperparameters and evaluate its performance on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeDngyS1tUeD"
      },
      "outputs": [],
      "source": [
        "# import libaries\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# initialize and train the decision tree model\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "dt_clf.fit(train_images, train_labels)\n",
        "\n",
        "# predict labels for the validation set\n",
        "val_set_pred = dt_clf.predict(val_images)\n",
        "\n",
        "# report the validation accuracy of the trained model\n",
        "val_accuracy = accuracy_score(val_labels, val_set_pred)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGSxnwEDYKDD"
      },
      "outputs": [],
      "source": [
        "# plot the learning curve of the trained model to examine bias and variance\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "\n",
        "    # Determine sizes and scores\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "\n",
        "    # Calculate the average and standard deviation of the training and test scores\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    # Plot the learning curve\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "\n",
        "\n",
        "plot_learning_curve(dt_clf, \"Decision Tree Learning Curve\", train_images, train_labels, cv=5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1gYivx4YQim"
      },
      "source": [
        "Does your model suffer from overfitting (high variance) or underfitting (high bias) or neither and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsN6JB_LYSjX"
      },
      "source": [
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrfFy74pDL1g"
      },
      "source": [
        "#### Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lLPA2wNcc8e"
      },
      "source": [
        "Use the **GridSearchCV** module of the **sklearn** library to tune the hyperparameter of your Decision Tree model on the validation set. You should try to tune as many hyperparameters as you can, such as split criterion, maximum depth, etc. Your tuning should be guided by the observations you made from the learning curve of the untuned model.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "midy9m00D3js"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 10, 20]\n",
        "}\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=3)\n",
        "grid_search.fit(train_images, train_labels)\n",
        "print(\"Best parameters:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OqO3THsKugF"
      },
      "source": [
        "### Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Go8gJ9pCHZ7"
      },
      "source": [
        "Train a Gradient Boosting model using the default hyperparameters and validate it on the validation set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Alzl5JabCG39"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# initialize and train the Gradient Boosting model\n",
        "gb_clf = GradientBoostingClassifier()\n",
        "gb_clf.fit(train_images, train_labels)\n",
        "\n",
        "# predict labels for the validation set\n",
        "val_set_pred = gb_clf.predict(val_images)\n",
        "\n",
        "# report the validation accuracy of the trained model\n",
        "val_accuracy = accuracy_score(val_labels, val_set_pred)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5PkBCw9LoOE"
      },
      "outputs": [],
      "source": [
        "# plot the learning curve of the trained model to examine bias and variance\n",
        "plot_learning_curve(gb_clf, \"Gradient Boosting Learning Curve\", train_images, train_labels, cv=5)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XAWse_7LoOP"
      },
      "source": [
        "Does your model suffer from overfitting (high variance) or underfitting (high bias) or neither and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXVqbo8OLoOP"
      },
      "source": [
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTg0wHAEKQ08"
      },
      "source": [
        "#### Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcPuJ6AkKQ1E"
      },
      "source": [
        "Use the **GridSearchCV** module of the **sklearn** library to tune the hyperparameter of your Gradient Boosting model on the validation set. You should try to tune as many hyperparameters as you can, such as criterion, maximum depth, learning rate, number of estimators, etc. Your tuning should be guided by the observations you made from the learning curve of the untuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeLRrmtdKQ1F"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 10, 20]\n",
        "}\n",
        "grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=3)\n",
        "grid_search.fit(train_images, train_labels)\n",
        "print(\"Best parameters:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrJ5vYnvDRSb"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3R5b8gNLTi_"
      },
      "source": [
        "Train a Random Forest model using the default hyperparameters and validate it on the validation set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r84fTJXDTXY"
      },
      "outputs": [],
      "source": [
        "# import libaries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# initialize and train the Random Forest model\n",
        "rf_clf = RandomForestClassifier()\n",
        "rf_clf.fit(train_images, train_labels)\n",
        "\n",
        "# predict labels for the validation set\n",
        "val_set_pred = rf_clf.predict(val_images)\n",
        "\n",
        "# report the validation accuracy of the trained model\n",
        "val_accuracy = accuracy_score(val_labels, val_set_pred)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HO18Cq1MRCY"
      },
      "outputs": [],
      "source": [
        "# plot the learning curve of the trained model to examine bias and variance\n",
        "plot_learning_curve(rf_clf, \"Random Forest Learning Curve\", train_images, train_labels, cv=5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmfSh0I0MRCj"
      },
      "source": [
        "Does your model suffer from overfitting (high variance) or underfitting (high bias) or neither and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2CPoLJnMRCj"
      },
      "source": [
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG6PgpqBLL3K"
      },
      "source": [
        "#### Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrhp74TOLL3S"
      },
      "source": [
        "Use the **GridSearchCV** module of the **sklearn** library to tune the hyperparameter of your decision tree model on the validation set. You should try to tune as many hyperparameters as you can, such as split criterion, maximum depth, etc. Your tuning should be guided by the observations you made from the learning curve of the untuned model. Your tuning should be guided by the observations you made from the learning curve of the untuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFYNhkXDLL3T"
      },
      "outputs": [],
      "source": [
        "# code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW_RqpkavvpR"
      },
      "source": [
        "### Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy0OC8cTMvZW"
      },
      "source": [
        "Train a linear Support Vector Machines model using the default hyperparameters and validate it on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H1FMrE_vuYK"
      },
      "outputs": [],
      "source": [
        "#code here\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# initialize and train the Support Vector Machines model\n",
        "svm_clf = SVC()\n",
        "svm_clf.fit(train_images, train_labels)\n",
        "\n",
        "# report the number of support vectors\n",
        "\n",
        "\n",
        "# predict labels for the validation set\n",
        "val_set_pred = svm_clf.predict(val_labels, val_set_pred)\n",
        "\n",
        "# report the validation accuracy of the trained model\n",
        "val_accuracy = accuracy_score(val_labels, val_set_pred)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGPJ-xAGMikg"
      },
      "outputs": [],
      "source": [
        "# plot the learning curve of the trained model to examine bias and variance\n",
        "plot_learning_curve(svm_clf, \"SVM Learning Curve\", train_images, train_labels, cv=5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxRCQX6CMikg"
      },
      "source": [
        "Does your model suffer from overfitting (high variance) or underfitting (high bias) or neither and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_fWauMkMikg"
      },
      "source": [
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj23miglMikg"
      },
      "source": [
        "#### Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj3erazVMikg"
      },
      "source": [
        "Use the **GridSearchCV** module of the **sklearn** library to tune the hyperparameter of your Support Vector Machines model on the validation set. You should try to tune as many hyperparameters as you can, such as the regularization parameter C, the kernel, the degree of polynomial kernels, and the gamma parameter. Your tuning should be guided by the observations you made from the learning curve of the untuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKiVLEl_Mikg"
      },
      "outputs": [],
      "source": [
        "# code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Jmt5yav5Ys"
      },
      "source": [
        "### Feed-forward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi54SPJNTzGQ"
      },
      "outputs": [],
      "source": [
        "# install necessary libraries if needed\n",
        "!pip install keras.wrappers\n",
        "!pip install tensorflow\n",
        "!pip install scikeras\n",
        "!pip install keras==2.15.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD9YRmWwOvuy"
      },
      "source": [
        "Train a Feed-forward Neural Network consisting of three layers. The first hidden layer has 128 units and uses the ReLU activation function, the second hidden layer has 64 units and uses the ReLU activation function, and the output layer has 1 unit and uses sigmoid activation for binary classification. Use the Adam optimizer to train the neural network, and then validate your model using the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1tudYQcwB_L"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# function to create a dense model\n",
        "def create_dense_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=input_shape),  # Flatten the input images\n",
        "        Dense(128, activation='relu'),  # First hidden layer with 128 units and ReLU activation\n",
        "        Dense(64, activation='relu'),   # Second hidden layer with 64 units and ReLU activation\n",
        "        Dense(1, activation='sigmoid')  # Output layer with 1 unit and sigmoid activation for binary classification\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# function to extract the dense model from the pipeline\n",
        "def extract_model(pipeline):\n",
        "    return pipeline.steps[-1][1]\n",
        "\n",
        "# Create a pipeline for the dense model\n",
        "dense_pipeline = Pipeline([\n",
        "    ('flatten', FunctionTransformer(lambda x: x.reshape((x.shape[0], -1)))),\n",
        "    ('dense', KerasClassifier(build_fn=create_dense_model, input_shape=(10000,), epochs=10, batch_size=32))\n",
        "])\n",
        "\n",
        "# train the network\n",
        "nn_model = create_model((train_images.shape[1],))\n",
        "history = nn_model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(val_images, val_labels))\n",
        "\n",
        "# predict labels for the validation set\n",
        "val_probabilities = nn_model.predict(val_images)\n",
        "val_set_pred = (val_probabilities > 0.5).astype(\"int32\")\n",
        "\n",
        "# report the validation accuracy of the trained model\n",
        "val_accuracy = accuracy_score(val_labels, val_set_pred)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkTrmBsmQ6YB"
      },
      "outputs": [],
      "source": [
        "# plot the learning curve of the trained model to examine bias and variance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Learning Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRKLCJpNQ6YM"
      },
      "source": [
        "Does your model suffer from overfitting (high variance) or underfitting (high bias) or neither and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LccO1xfHQ6YM"
      },
      "source": [
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOkCypDdQ6YM"
      },
      "source": [
        "#### Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB2vxapgQ6YM"
      },
      "source": [
        "Use the **RandomizedSearchCV** module of the **sklearn** library to tune the hyperparameter of your Neural Network model on the validation set. You should try to tune as many hyperparameters as you can, such as the learning rate, the number of hidden units, batch size, learning rate decay, and the number of hidden layers. Your tuning should be guided by the observations you made from the learning curve of the untuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zelRwwPfR4sW"
      },
      "outputs": [],
      "source": [
        "# code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sRbdUhSwEhK"
      },
      "source": [
        "## Part 4: Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXSwj5COYOat"
      },
      "source": [
        "Select the best tuned model from all the five models you have trained in part 4, and then test it on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAWEr8nYwKfQ"
      },
      "outputs": [],
      "source": [
        "# Predict labels for the test set using the best model\n",
        "test_predictions = best_model.predict(test_images)\n",
        "\n",
        "# Report the test accuracy of the trained model\n",
        "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
