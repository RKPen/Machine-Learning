{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf8zksoawaHq"
      },
      "source": [
        "## Assignment 2: Sentiment Analysis\n",
        "\n",
        "The goal of this assignment is to train a machine learning model that can predict the sentiment of short statements as either negative, positive or neutral. To train your model, split the attached training data into training, validation and test sets, and train a multi-class logistic regression classifier using the training data. Tune the hyperparameters of the model using the validation set, and finally once all the hyperparameters are tuned and you selected your best model, test it on the test data and report its performance using the different metrics used for evaluating multi-class classifiers.\n",
        "\n",
        "*Note: Please include comments to your code so it can be easily followed and understood.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxFKBF_4xByj"
      },
      "source": [
        "### Loading the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7PmButs0TOj"
      },
      "source": [
        "Load the data.csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg2D2bbw0c5c"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s0DbkyuxivW"
      },
      "outputs": [],
      "source": [
        "# load the data.csv file on moodle\n",
        "df = pd.read_csv('data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e-9xhC4xxDp"
      },
      "outputs": [],
      "source": [
        "# display the first 5 rows of your dataset so you can explore its content\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTToaZ9WWXWk"
      },
      "source": [
        "### Cleaning the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGX60Y3mWzSa"
      },
      "source": [
        "Remove duplicates and missing values from your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KK0kzUJW6GL"
      },
      "outputs": [],
      "source": [
        "# check for missing values\n",
        "print(\"\\nChecking for missing values:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXVHVMnDW-Ua"
      },
      "outputs": [],
      "source": [
        "# drop missing values\n",
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxzweFphXBE5"
      },
      "outputs": [],
      "source": [
        "# check for duplicates\n",
        "print(\"\\nChecking for duplicates:\")\n",
        "print(df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_mIdKR9XENn"
      },
      "outputs": [],
      "source": [
        "# drop duplicates\n",
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeOgdKiqWB8E"
      },
      "source": [
        "### Splitting the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBbiZa8nWIPE"
      },
      "source": [
        "Split your dataset into train (80%), validation (10%) and test (10%) sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikt_b7wxWP-P"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2amSQIinWRym"
      },
      "outputs": [],
      "source": [
        "# split the data\n",
        "total_samples = len(df)   \n",
        "\n",
        "X = df['text']\n",
        "y = df['sentiment']\n",
        "\n",
        "# Split the data into train and temp sets (80% train, 20% temp)\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Split the temp set into validation and final test sets (50% each)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.50, random_state=42)\n",
        "\n",
        "# Display the shapes of the resulting sets\n",
        "print(\"Train set shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
        "print(\"Test set shape:\", X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StQdiBQiXjbJ"
      },
      "source": [
        "### Pre-processing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBxC2fKLDL1e"
      },
      "source": [
        " Pre-process your data by converting all characters to lowercase, removing stop words and performing stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSfgLxPXDL1e"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "%pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuNUr_EJDL1f"
      },
      "outputs": [],
      "source": [
        "# pre-process the data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove stop words\n",
        "    words = [word for word in text.split() if word.lower() not in stop_words]\n",
        "    \n",
        "    # Perform stemming\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    \n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing to the 'text' column\n",
        "df['text'] = df['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBudMp-zDL1f"
      },
      "source": [
        "### Representing the data using TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgn2eY3VXlpR"
      },
      "source": [
        "Transform your pre-processed data into tf-idf vectors using the **TfidfVectorizer** of the **sklearn** library. *Note:* You should first transform the training data into TF-IDF vectors, and then when transforming the validation and test data, you should make sure that the IDF scores for test instances are deduced from the training data to avoid data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHbc5Pk5ZUhm"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TfidfVectorizer \n",
        "tfidf_vectorizer = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EixI2HTCZahB"
      },
      "outputs": [],
      "source": [
        "# transform the text column of the training data into TF-IDF vectors\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PbrsBfdaVPI"
      },
      "outputs": [],
      "source": [
        "# transform the text column of the validation data into TF-IDF vectors\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj0Z04u8aV2_"
      },
      "outputs": [],
      "source": [
        "# transform the text column of the test data into TF-IDF vectors\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7e70o39x6h4"
      },
      "source": [
        "### Training a classifier\n",
        "\n",
        "Train a multi-class logistic regression model to predict the sentiment of statements into either negative, positive or neutral.You should use the **\"sentiment\"** column as the target variable and all the remaining **TF-IDF features**' columns created above as independent variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2VvRdAQ1KRv"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRxtNID-bkY3"
      },
      "outputs": [],
      "source": [
        "# train the logistic regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#y_train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZuuAIgab9C_"
      },
      "outputs": [],
      "source": [
        "# report the accuracy of the model on your validation set\n",
        "val_predictions = model.predict(X_val_tfidf)\n",
        "accuracy = accuracy_score(y_val, val_predictions)\n",
        "print(\"Accuracy on Validation Set:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GPyac1FcExk"
      },
      "outputs": [],
      "source": [
        "# plot the learning curve of the model to examine bias and variance\n",
        "train_sizes, train_scores, val_scores = learning_curve(model, X_train_tfidf, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training Accuracy')\n",
        "plt.plot(train_sizes, np.mean(val_scores, axis=1), label='Validation Accuracy')\n",
        "plt.title('Learning Curve')\n",
        "plt.xlabel('Number of Training Samples')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VZav6hqcaEw"
      },
      "source": [
        "Does your model suffer from overfitting (high variance) or underfitting (high bias) or neither and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMc9WO4DDL1g"
      },
      "source": [
        "Answer: The model seems okay as the training accuracy is in a reasonable range and seems to slowly start tending to 1 with increasing Number of training samples. Furthermore, The validation accuracy curve increases sharply in the beginning indicating that the model is learning but then plateau's at 0.5 and increases very very slowly with larger training samples.\n",
        "Since we are not getting a lot of information by adding new samples, this may suggest that there may be a lack of generalization (possible overfitting).\n",
        "However all in all, the model seems to suffer from neither.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrfFy74pDL1g"
      },
      "source": [
        "### Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lLPA2wNcc8e"
      },
      "source": [
        "Use the **GridSearchCV** module of the **sklearn** library to tune the hyperparameter of your logistic regression model on the validation set. You should try to tune as many hyperparameters as you can, such as regularization, learning rate, solver, number of iterations, etc. Your tuning should be guided by the observations you made from the learning curve of the untuned model.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzGhn8kWcyUf"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqX6sJ3Vc2gX"
      },
      "outputs": [],
      "source": [
        "# use gridsearch to find the best combination of hyperparameters\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'penalty': ['l2'],           # Regularization type\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10],    # Inverse of regularization strength\n",
        "    'solver': ['lbfgs'],  # Optimization algorithm\n",
        "    'max_iter': [100, 500, 1000],       # Maximum number of iterations\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#To test the highest score\n",
        "print('Best score: ', grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li_0cP5vd1B1"
      },
      "source": [
        "### Model Selection and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auM-Qpd1d5JD"
      },
      "source": [
        "Choose the best model with the best hyperparameters based on the validation set and test on the test data using accuracy, precision, recall, and F1-score. Display all these evaluation metrics as well as the confusion matrix of the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAwn9ug8OkRO"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyrNGL-OdACV"
      },
      "outputs": [],
      "source": [
        "# validate the accuracy of your best selected model\n",
        "\n",
        "\n",
        "# Best hyperparameters from the previous part\n",
        "best_hyperparameters = {'C': 10, 'max_iter': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
        "\n",
        "# Create the best model with the best hyperparameters\n",
        "best_model = LogisticRegression(**best_hyperparameters)\n",
        "\n",
        "# Train the best model on the entire training data\n",
        "best_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions_best = best_model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "test_accuracy_best = accuracy_score(y_test, test_predictions_best)\n",
        "test_precision_best = precision_score(y_test, test_predictions_best, average='weighted')\n",
        "test_recall_best = recall_score(y_test, test_predictions_best, average='weighted')\n",
        "test_f1_score_best = f1_score(y_test, test_predictions_best, average='weighted')\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Evaluation Metrics for the Best Model on Test Set:\")\n",
        "print(\"Accuracy:\", test_accuracy_best)\n",
        "print(\"Precision:\", test_precision_best)\n",
        "print(\"Recall:\", test_recall_best)\n",
        "print(\"F1-Score:\", test_f1_score_best)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
